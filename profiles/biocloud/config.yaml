#command with which to start SLURM jobs
cluster:
  mkdir -p logs/{rule}/ &&
  sbatch
    --parsable
    --partition={resources.partition}
    --qos={resources.qos}
    --cpus-per-task={threads}
    --mem={resources.mem_mb}
    --job-name=smk-{rule}-{wildcards}
    --output=logs/{rule}/{rule}-{wildcards}-%j.out
#if rules don't have resources set, use these default values
default-resources:
  - partition="general"
  - qos="normal"
  - threads=128
  - mem_mb=1024
#max threads per job/rule. Will take precedence over anything else. Adjust this
#before submitting to SLURM and leave threads settings elsewhere untouched
max-threads: 32
use-conda: True
conda-frontend: mamba
printshellcmds: False
jobs: 50
local-cores: 1
latency-wait: 120
restart-times: 1
max-jobs-per-second: 10
keep-going: True
rerun-incomplete: True
scheduler: greedy
max-status-checks-per-second: 5
cluster-cancel: scancel
cluster-status: extras/slurm-status.sh #script to get job status for snakemake
